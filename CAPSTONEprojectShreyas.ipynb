{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Untitled0.ipynb","provenance":[{"file_id":"1QJhacewnCNHlOfuIzUxS9qdTkGcub6R1","timestamp":1660661713220}],"collapsed_sections":[],"mount_file_id":"1Tuxj4fnDsqkb6vmdy-GIycJVPfXPgZy2","authorship_tag":"ABX9TyOMHpWtg7oTR8kFCwYhjmUV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"5eFB_IQlBU_5","executionInfo":{"status":"ok","timestamp":1660747897462,"user_tz":240,"elapsed":9,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}}},"outputs":[],"source":["# Function that commits the contents of the dataset to memory.\n","\n","def ds_commit(dataset):\n","\n","  # Opening the file in a \"read only\" format and encoding it using the UTF-8 method.\n","  file = open(dataset, mode = \"rt\", encoding = \"utf-8\")\n","\n","  # Reading the contents of the dataset.\n","  df_contents = file.read()\n","\n","  # Close the dataset.\n","  file.close()\n","\n","  return df_contents"]},{"cell_type":"code","source":[" # Mounting the google drive onto Google collaboratory.\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWjDalN5Zk1S","executionInfo":{"status":"ok","timestamp":1660747907999,"user_tz":240,"elapsed":8099,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}},"outputId":"05b79198-7493-4f29-fad3-6edebff4f3a4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Splitting the text conjured in the function above into phrases.\n","\n","def to_phrase(df):\n","\n","  # Getting rid of white space and grouping the phrases.\n","  lines = df.strip().split(\"\\n\")\n","  tr_sets = [line.split(\"\\t\") for line in lines]\n","  return tr_sets"],"metadata":{"id":"Q6lZsA4cSz24","executionInfo":{"status":"ok","timestamp":1660747909566,"user_tz":240,"elapsed":325,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import string\n","import re\n","import numpy as np\n","from pickle import dump\n","from pickle import load\n","from numpy.random import rand\n","from numpy.random import shuffle\n","from unicodedata import normalize\n","\n","\n","def clean_pairs(lines):\n","\tcleaned = list()\n","\t# Creating ReGex instance for character filtering.\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n"," \n","\t# Forming a translation table for removing punctuation marks.\n","\ttable = str.maketrans('', '', string.punctuation)\n"," \n","\tfor pair in lines:\n","\t\tclean_pair = list()\n","\t\tfor line in pair:\n","\t\t\t\n","      \n","      \n","      # Normalizing unicode characters.\n","\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n","\t\t\tline = line.decode('UTF-8')\n","   \n","\t\t\t# Tokenizing the phrases.\n","\t\t\tline = line.split()\n","   \n","\t\t\t# Changing all letters into to their lowercase form.\n","\t\t\tline = [word.lower() for word in line]\n","\n","\t\t\t# Omitting the punctuation marks from each token.\n","\t\t\tline = [word.translate(table) for word in line]\n","\n","\t\t\t# Omitting all non-printable characters from each token.\n","\t\t\tline = [re_print.sub('', w) for w in line]\n","\n","\t\t\t# Omitting tokens with numbers.\n","\t\t\tline = [word for word in line if word.isalpha()]\n","\n","\t\t\t# Storing the cleaned phrases in a list and converting it into an array.\n","\t\t\tclean_pair.append(' '.join(line))\n","\t\tcleaned.append(clean_pair)\n","\treturn np.array(cleaned)"],"metadata":{"id":"vzrzjPZqt_6a","executionInfo":{"status":"ok","timestamp":1660747912645,"user_tz":240,"elapsed":2,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Saving the cleaned phrases to a new file.\n","\n","def save_clean_data(phrases, filename):\n","\tdump(phrases, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)"],"metadata":{"id":"44kkGgU4IjX-","executionInfo":{"status":"ok","timestamp":1660747916817,"user_tz":240,"elapsed":316,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Importing dataset and extracting the contents using the \"df_commit\" function.\n","dataset = '/content/drive/MyDrive/deu.txt'\n","df = ds_commit(dataset)"],"metadata":{"id":"eI-YEPerKq4e","executionInfo":{"status":"ok","timestamp":1660747919543,"user_tz":240,"elapsed":1124,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"xy7pej9dZakw","executionInfo":{"status":"ok","timestamp":1660747921561,"user_tz":240,"elapsed":695,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}}},"outputs":[],"source":["# Splitting the contents into groups of english phrases and their corresponding german translations (\"to_phrase\" function).\n","EGpairs = to_phrase(df)"]},{"cell_type":"code","source":["# Cleaning the phrases of unnecessary elements as described in the \"clean_pairs\" function.\n","clean_df = clean_pairs(EGpairs)"],"metadata":{"id":"yt9CfF3iPjEP","executionInfo":{"status":"ok","timestamp":1660747927467,"user_tz":240,"elapsed":4049,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Saving the cleaned phrases file (as a pkl file)\n","save_clean_data(clean_df, 'eng-german-transl.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hxcgat7jsrG5","executionInfo":{"status":"ok","timestamp":1660747928928,"user_tz":240,"elapsed":1464,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}},"outputId":"182e96c7-bffa-4cde-9727-4d9aea017d73"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: eng-german-transl.pkl\n"]}]},{"cell_type":"code","source":["# spot check \n","for i in range(100):\n","\tprint('[%s] => [%s]' % (clean_df[i,0], clean_df[i,1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAU7hHiltQR8","executionInfo":{"status":"ok","timestamp":1660422774097,"user_tz":240,"elapsed":218,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}},"outputId":"c18627dc-5ee3-494c-cde3-5fb40a4fa2ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[hi] => [hallo]\n","[hi] => [gru gott]\n","[run] => [lauf]\n","[wow] => [potzdonner]\n","[wow] => [donnerwetter]\n","[fire] => [feuer]\n","[help] => [hilfe]\n","[help] => [zu hulf]\n","[stop] => [stopp]\n","[wait] => [warte]\n","[hello] => [hallo]\n","[i try] => [ich probiere es]\n","[i won] => [ich hab gewonnen]\n","[i won] => [ich habe gewonnen]\n","[smile] => [lacheln]\n","[cheers] => [zum wohl]\n","[freeze] => [keine bewegung]\n","[freeze] => [stehenbleiben]\n","[got it] => [verstanden]\n","[got it] => [einverstanden]\n","[he ran] => [er rannte]\n","[he ran] => [er lief]\n","[hop in] => [mach mit]\n","[hug me] => [druck mich]\n","[hug me] => [nimm mich in den arm]\n","[hug me] => [umarme mich]\n","[i fell] => [ich fiel]\n","[i fell] => [ich fiel hin]\n","[i fell] => [ich sturzte]\n","[i fell] => [ich bin hingefallen]\n","[i fell] => [ich bin gesturzt]\n","[i know] => [ich wei]\n","[i lied] => [ich habe gelogen]\n","[i lost] => [ich habe verloren]\n","[im] => [ich bin jahre alt]\n","[im] => [ich bin]\n","[im ok] => [mir gehts gut]\n","[im ok] => [es geht mir gut]\n","[no way] => [unmoglich]\n","[no way] => [das gibts doch nicht]\n","[no way] => [ausgeschlossen]\n","[no way] => [in keinster weise]\n","[really] => [wirklich]\n","[really] => [echt]\n","[really] => [im ernst]\n","[thanks] => [danke]\n","[try it] => [versuchs]\n","[why me] => [warum ich]\n","[ask tom] => [frag tom]\n","[ask tom] => [fragen sie tom]\n","[ask tom] => [fragt tom]\n","[be cool] => [entspann dich]\n","[be fair] => [sei nicht ungerecht]\n","[be fair] => [sei fair]\n","[be nice] => [sei nett]\n","[be nice] => [seien sie nett]\n","[beat it] => [geh weg]\n","[beat it] => [hau ab]\n","[beat it] => [verschwinde]\n","[beat it] => [verdufte]\n","[beat it] => [mach dich fort]\n","[beat it] => [zieh leine]\n","[beat it] => [mach dich vom acker]\n","[beat it] => [verzieh dich]\n","[beat it] => [verkrumele dich]\n","[beat it] => [troll dich]\n","[beat it] => [zisch ab]\n","[beat it] => [pack dich]\n","[beat it] => [mach ne fliege]\n","[beat it] => [schwirr ab]\n","[beat it] => [mach die sause]\n","[beat it] => [scher dich weg]\n","[beat it] => [scher dich fort]\n","[call me] => [ruf mich an]\n","[come in] => [komm herein]\n","[come in] => [herein]\n","[come on] => [komm]\n","[come on] => [kommt]\n","[come on] => [mach schon]\n","[come on] => [macht schon]\n","[get out] => [raus]\n","[go away] => [geh weg]\n","[go away] => [hau ab]\n","[go away] => [verschwinde]\n","[go away] => [verdufte]\n","[go away] => [mach dich fort]\n","[go away] => [zieh leine]\n","[go away] => [mach dich vom acker]\n","[go away] => [verzieh dich]\n","[go away] => [verkrumele dich]\n","[go away] => [troll dich]\n","[go away] => [zisch ab]\n","[go away] => [pack dich]\n","[go away] => [mach ne fliege]\n","[go away] => [schwirr ab]\n","[go away] => [mach die sause]\n","[go away] => [scher dich weg]\n","[go away] => [scher dich fort]\n","[go away] => [geh weg]\n","[go away] => [verpiss dich]\n"]}]},{"cell_type":"code","source":["# Fetching the cleaned data.\n","\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))"],"metadata":{"id":"C4OxfYf_qoCV","executionInfo":{"status":"ok","timestamp":1660747933993,"user_tz":240,"elapsed":5,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Loading the compiled and cleaned dataset\n","comp_dataset = load_clean_sentences('eng-german-transl.pkl')\n"," \n","# Reducing the dataset size to save computation time.\n","n_phrases = 35000\n","dataset = comp_dataset[:n_phrases, :]\n","\n","# Randomly shuffling data in order to eliminate any sort of \"order\".\n","shuffle(dataset)\n","\n","# Splitting the data in the training and testing set.\n","train, test = dataset[:31500], dataset[31500:]\n","\n","# Saving the split data as pkl files.\n","save_clean_data(dataset, 'eng-german-comp.pkl')\n","save_clean_data(train, 'eng-german-train.pkl')\n","save_clean_data(test, 'eng-german-test.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5sPh0oGsuJFC","executionInfo":{"status":"ok","timestamp":1660747939107,"user_tz":240,"elapsed":1508,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}},"outputId":"72262071-7409-45e7-a87b-7f150ec7eb9d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: eng-german-comp.pkl\n","Saved: eng-german-train.pkl\n","Saved: eng-german-test.pkl\n"]}]},{"cell_type":"code","source":["from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","from keras.layers import Embedding\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.callbacks import ModelCheckpoint\n"," \n","# Loading the cleaned dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n"," \n","# Creating a tokenizer\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n"," \n","# Function to determine the length of the longest phrase (enabled by the .fit_on_texts function).\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)\n"," \n","# Encoding the sequences and then padding them with zeroes.\n","def encode_sequences(tokenizer, length, lines):\n","\t# integer encode sequences\n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n"," \n","# One hot encoding the output english phrases.\n","def encode_output(sequences, vocab_size):\n","\tylist = list()\n","\tfor sequence in sequences:\n","\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n","\t\tylist.append(encoded)\n","\ty = array(ylist)\n","\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n","\treturn y\n"," \n","# define translation model (LSTM)\n","def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n","\tmodel = Sequential()\n","\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n","\tmodel.add(LSTM(n_units))\n","\tmodel.add(RepeatVector(tar_timesteps))\n","\tmodel.add(LSTM(n_units, return_sequences=True))\n","\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n","\treturn model\n"," \n","# Loading the training, testing and compiled datasets datasets\n","dataset = load_clean_sentences('eng-german-comp.pkl')\n","train = load_clean_sentences('eng-german-train.pkl')\n","test = load_clean_sentences('eng-german-test.pkl')\n"," \n","# Forming the tokenizers for the english and their corresponding german phrases.\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","print('English Vocabulary Size: %d' % eng_vocab_size)\n","print('English Max Length: %d' % (eng_length))\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","print('German Vocabulary Size: %d' % ger_vocab_size)\n","print('German Max Length: %d' % (ger_length))\n"," \n","# Prepping the training and testing data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n","trainY = encode_output(trainY, eng_vocab_size)\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n","testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n","testY = encode_output(testY, eng_vocab_size)\n"," \n","# Defining the model.\n","model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","\n","# Model summary.\n","print(model.summary())\n","plot_model(model, to_file='model.png', show_shapes=True)\n","\n","# Fitting the model.\n","filename = 'model.h5'\n","checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iGeXewDq8Ld7","executionInfo":{"status":"ok","timestamp":1660675539829,"user_tz":240,"elapsed":4660325,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}},"outputId":"757fbdf2-d203-46a3-8740-10a34140c477"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["English Vocabulary Size: 5416\n","English Max Length: 7\n","German Vocabulary Size: 9011\n","German Max Length: 17\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 17, 256)           2306816   \n","                                                                 \n"," lstm (LSTM)                 (None, 256)               525312    \n","                                                                 \n"," repeat_vector (RepeatVector  (None, 7, 256)           0         \n"," )                                                               \n","                                                                 \n"," lstm_1 (LSTM)               (None, 7, 256)            525312    \n","                                                                 \n"," time_distributed (TimeDistr  (None, 7, 5416)          1391912   \n"," ibuted)                                                         \n","                                                                 \n","=================================================================\n","Total params: 4,749,352\n","Trainable params: 4,749,352\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/30\n","\n","Epoch 1: val_loss improved from inf to 3.45123, saving model to model.h5\n","493/493 - 164s - loss: 3.8313 - val_loss: 3.4512 - 164s/epoch - 333ms/step\n","Epoch 2/30\n","\n","Epoch 2: val_loss improved from 3.45123 to 3.26843, saving model to model.h5\n","493/493 - 155s - loss: 3.3125 - val_loss: 3.2684 - 155s/epoch - 314ms/step\n","Epoch 3/30\n","\n","Epoch 3: val_loss improved from 3.26843 to 3.06071, saving model to model.h5\n","493/493 - 157s - loss: 3.0983 - val_loss: 3.0607 - 157s/epoch - 319ms/step\n","Epoch 4/30\n","\n","Epoch 4: val_loss improved from 3.06071 to 2.85475, saving model to model.h5\n","493/493 - 156s - loss: 2.8558 - val_loss: 2.8548 - 156s/epoch - 317ms/step\n","Epoch 5/30\n","\n","Epoch 5: val_loss improved from 2.85475 to 2.58042, saving model to model.h5\n","493/493 - 156s - loss: 2.5791 - val_loss: 2.5804 - 156s/epoch - 316ms/step\n","Epoch 6/30\n","\n","Epoch 6: val_loss improved from 2.58042 to 2.37496, saving model to model.h5\n","493/493 - 156s - loss: 2.2901 - val_loss: 2.3750 - 156s/epoch - 316ms/step\n","Epoch 7/30\n","\n","Epoch 7: val_loss improved from 2.37496 to 2.19755, saving model to model.h5\n","493/493 - 156s - loss: 2.0543 - val_loss: 2.1975 - 156s/epoch - 316ms/step\n","Epoch 8/30\n","\n","Epoch 8: val_loss improved from 2.19755 to 2.07665, saving model to model.h5\n","493/493 - 156s - loss: 1.8453 - val_loss: 2.0767 - 156s/epoch - 317ms/step\n","Epoch 9/30\n","\n","Epoch 9: val_loss improved from 2.07665 to 1.96934, saving model to model.h5\n","493/493 - 157s - loss: 1.6550 - val_loss: 1.9693 - 157s/epoch - 317ms/step\n","Epoch 10/30\n","\n","Epoch 10: val_loss improved from 1.96934 to 1.86826, saving model to model.h5\n","493/493 - 156s - loss: 1.4839 - val_loss: 1.8683 - 156s/epoch - 317ms/step\n","Epoch 11/30\n","\n","Epoch 11: val_loss improved from 1.86826 to 1.79749, saving model to model.h5\n","493/493 - 156s - loss: 1.3290 - val_loss: 1.7975 - 156s/epoch - 316ms/step\n","Epoch 12/30\n","\n","Epoch 12: val_loss improved from 1.79749 to 1.71525, saving model to model.h5\n","493/493 - 155s - loss: 1.1860 - val_loss: 1.7152 - 155s/epoch - 315ms/step\n","Epoch 13/30\n","\n","Epoch 13: val_loss improved from 1.71525 to 1.66573, saving model to model.h5\n","493/493 - 155s - loss: 1.0551 - val_loss: 1.6657 - 155s/epoch - 315ms/step\n","Epoch 14/30\n","\n","Epoch 14: val_loss improved from 1.66573 to 1.62904, saving model to model.h5\n","493/493 - 155s - loss: 0.9360 - val_loss: 1.6290 - 155s/epoch - 315ms/step\n","Epoch 15/30\n","\n","Epoch 15: val_loss improved from 1.62904 to 1.59037, saving model to model.h5\n","493/493 - 155s - loss: 0.8338 - val_loss: 1.5904 - 155s/epoch - 315ms/step\n","Epoch 16/30\n","\n","Epoch 16: val_loss improved from 1.59037 to 1.56486, saving model to model.h5\n","493/493 - 157s - loss: 0.7408 - val_loss: 1.5649 - 157s/epoch - 319ms/step\n","Epoch 17/30\n","\n","Epoch 17: val_loss improved from 1.56486 to 1.55233, saving model to model.h5\n","493/493 - 155s - loss: 0.6616 - val_loss: 1.5523 - 155s/epoch - 314ms/step\n","Epoch 18/30\n","\n","Epoch 18: val_loss improved from 1.55233 to 1.54996, saving model to model.h5\n","493/493 - 153s - loss: 0.5914 - val_loss: 1.5500 - 153s/epoch - 311ms/step\n","Epoch 19/30\n","\n","Epoch 19: val_loss improved from 1.54996 to 1.54840, saving model to model.h5\n","493/493 - 152s - loss: 0.5291 - val_loss: 1.5484 - 152s/epoch - 308ms/step\n","Epoch 20/30\n","\n","Epoch 20: val_loss improved from 1.54840 to 1.54611, saving model to model.h5\n","493/493 - 152s - loss: 0.4776 - val_loss: 1.5461 - 152s/epoch - 308ms/step\n","Epoch 21/30\n","\n","Epoch 21: val_loss did not improve from 1.54611\n","493/493 - 152s - loss: 0.4299 - val_loss: 1.5519 - 152s/epoch - 307ms/step\n","Epoch 22/30\n","\n","Epoch 22: val_loss did not improve from 1.54611\n","493/493 - 152s - loss: 0.3898 - val_loss: 1.5563 - 152s/epoch - 308ms/step\n","Epoch 23/30\n","\n","Epoch 23: val_loss did not improve from 1.54611\n","493/493 - 153s - loss: 0.3535 - val_loss: 1.5626 - 153s/epoch - 309ms/step\n","Epoch 24/30\n","\n","Epoch 24: val_loss did not improve from 1.54611\n","493/493 - 152s - loss: 0.3207 - val_loss: 1.5757 - 152s/epoch - 308ms/step\n","Epoch 25/30\n","\n","Epoch 25: val_loss did not improve from 1.54611\n","493/493 - 152s - loss: 0.2935 - val_loss: 1.5811 - 152s/epoch - 308ms/step\n","Epoch 26/30\n","\n","Epoch 26: val_loss did not improve from 1.54611\n","493/493 - 153s - loss: 0.2690 - val_loss: 1.6050 - 153s/epoch - 310ms/step\n","Epoch 27/30\n","\n","Epoch 27: val_loss did not improve from 1.54611\n","493/493 - 155s - loss: 0.2472 - val_loss: 1.6086 - 155s/epoch - 314ms/step\n","Epoch 28/30\n","\n","Epoch 28: val_loss did not improve from 1.54611\n","493/493 - 152s - loss: 0.2274 - val_loss: 1.6212 - 152s/epoch - 309ms/step\n","Epoch 29/30\n","\n","Epoch 29: val_loss did not improve from 1.54611\n","493/493 - 155s - loss: 0.2106 - val_loss: 1.6403 - 155s/epoch - 314ms/step\n","Epoch 30/30\n","\n","Epoch 30: val_loss did not improve from 1.54611\n","493/493 - 153s - loss: 0.1956 - val_loss: 1.6449 - 153s/epoch - 310ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f86e8c10a90>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["from pickle import load\n","from numpy import array\n","from numpy import argmax\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n"," \n","# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n"," \n","# fit a tokenizer\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n"," \n","# max sentence length\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)\n"," \n","# encode and pad sequences\n","def encode_sequences(tokenizer, length, lines):\n","\t# integer encode sequences\n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n"," \n","# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n"," \n","# generate target given source sequence\n","def predict_sequence(model, tokenizer, source):\n","\tprediction = model.predict(source, verbose=0)[0]\n","\tintegers = [argmax(vector) for vector in prediction]\n","\ttarget = list()\n","\tfor i in integers:\n","\t\tword = word_for_id(i, tokenizer)\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\ttarget.append(word)\n","\treturn ' '.join(target)\n"," \n","# evaluate the skill of the model\n","def evaluate_model(model, tokenizer, sources, raw_dataset):\n","\tactual, predicted = list(), list()\n","\tfor i, source in enumerate(sources):\n","\t\t# translate encoded source text\n","\t\tsource = source.reshape((1, source.shape[0]))\n","\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n","\t\traw_target, raw_src = raw_dataset[i]\n","\t\tif i < 10:\n","\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n","\t\tactual.append([raw_target.split()])\n","\t\tpredicted.append(translation.split())\n","\t# calculate BLEU score\n","\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"," \n","# load datasets\n","dataset = load_clean_sentences('eng-german-comp.pkl')\n","train = load_clean_sentences('eng-german-train.pkl')\n","test = load_clean_sentences('eng-german-test.pkl')\n","# prepare english tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","# prepare german tokenizer\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","# prepare data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n"," \n","# load model\n","model = load_model('Bestmodel.h5')\n","# test on some training sequences\n","#print('train')\n","#evaluate_model(model, eng_tokenizer, trainX, train)\n","# test on some test sequences\n","print('test')\n","evaluate_model(model, eng_tokenizer, testX, test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o2wZgfWyamFj","executionInfo":{"status":"ok","timestamp":1660750896314,"user_tz":240,"elapsed":201377,"user":{"displayName":"Shreyas Gowrishanker","userId":"10469478055378522884"}},"outputId":"03c7c4cb-5848-4fa6-82b0-697b996e00c2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["test\n","src=[es ist spat], target=[its late], predicted=[its theyre]\n","src=[ich bin tvsuchtig], target=[im a tv addict], predicted=[im a]\n","src=[ich habe zu viele taschen], target=[i have too many bags], predicted=[i am used to]\n","src=[tom ist dreiig], target=[tom is thirty], predicted=[tom is a]\n","src=[bitte geht], target=[please go], predicted=[please go]\n","src=[er erwiderte kein wort], target=[he made no response], predicted=[he will not]\n","src=[habe ich das erwahnt], target=[did i mention that], predicted=[did i girls this]\n","src=[haben sie ihren brief bekommen], target=[did you get her letter], predicted=[have you run your told]\n","src=[ich ziehe mich aus], target=[im undressing], predicted=[im dreaming to]\n","src=[ich werde spater bezahlen], target=[ill pay later], predicted=[ill be miss hear]\n","BLEU-1: 0.364155\n","BLEU-2: 0.225690\n","BLEU-3: 0.178976\n","BLEU-4: 0.096772\n"]}]}]}